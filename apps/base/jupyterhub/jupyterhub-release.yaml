apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: jupyterhub
  namespace: hub
spec:
  releaseName: jupyterhub
  targetNamespace: hub
  chart:
    spec:
      chart: jupyterhub
      version: "3.3.6"
      sourceRef:
        kind: HelmRepository
        name: jupyterhub
  interval: 5m0s
  # Default values
  # https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/main/jupyterhub/values.yaml
  values:
    ingress: {}
      # enabled: true
      # ingressClassName: nginx
      # annotations:
      #   cert-manager.io/cluster-issuer: letsencrypt-production
      #   external-dns.alpha.kubernetes.io/hostname: hub.org.datacube.world
      #   nginx.ingress.kubernetes.io/proxy-body-size: "0"
      #   nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      #   nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
      # hosts:
      #   - hub.org.datacube.world
      # tls:
      #   - hosts:
      #       - hub.org.datacube.world
      #     secretName: jupyterhub-ingress
    # fullnameOverride and nameOverride distinguishes blank strings, null values,
    # and non-blank strings. For more details, see the configuration reference.
    fullnameOverride: ""
    nameOverride:

    # enabled is ignored by the jupyterhub chart itself, but a chart depending on
    # the jupyterhub chart conditionally can make use this config option as the
    # condition.
    enabled:

    # custom can contain anything you want to pass to the hub pod, as all passed
    # Helm template values will be made available there.
    custom: {}

    # imagePullSecret is configuration to create a k8s Secret that Helm chart's pods
    # can get credentials from to pull their images.
    imagePullSecret:
      create: false
      automaticReferenceInjection: true
      registry:
      username:
      password:
      email:
        # imagePullSecrets is configuration to reference the k8s Secret resources the
        # Helm chart's pods can get credentials from to pull their images.
    imagePullSecrets: []

    # hub relates to the hub pod, responsible for running JupyterHub, its configured
    # Authenticator class KubeSpawner, and its configured Proxy class
    # ConfigurableHTTPProxy. KubeSpawner creates the user pods, and
    # ConfigurableHTTPProxy speaks with the actual ConfigurableHTTPProxy server in
    # the proxy pod.
    hub:
      revisionHistoryLimit:
      config:
        JupyterHub:
          admin_access: true
          authenticator_class: auth0
          admin_users:
          - alexgleith@gmail.com
        # Injected by flux
        # Auth0OAuthenticator:
        # ...
      service:
        type: ClusterIP
        annotations: {}
        ports:
          nodePort:
        extraPorts: []
        loadBalancerIP:
      baseUrl: /
      cookieSecret:
      initContainers: []
      nodeSelector: {}
      tolerations: []
      concurrentSpawnLimit: 64
      consecutiveFailureLimit: 5
      activeServerLimit:
      deploymentStrategy:
        ## type: Recreate
        ## - sqlite-pvc backed hubs require the Recreate deployment strategy as a
        ##   typical PVC storage can only be bound to one pod at the time.
        ## - JupyterHub isn't designed to support being run in parallell. More work
        ##   needs to be done in JupyterHub itself for a fully highly available (HA)
        ##   deployment of JupyterHub on k8s is to be possible.
        type: Recreate
      # Below is set in Terraform
      # db:
      #   type: sqlite-pvc
      #   upgrade:
      #   pvc:
      #     annotations: {}
      #     selector: {}
      #     accessModes:
      #       - ReadWriteOnce
      #     storage: 1Gi
      #     subPath:
      #   url:
      #   password:
      labels: {}
      annotations: {}
      command: []
      args: []
      extraConfig:
        01-add-dask-gateway-values: |
          c.KubeSpawner.environment["DASK_GATEWAY__ADDRESS"] = "http://traefik-dask-gateway"
          c.KubeSpawner.environment["DASK_GATEWAY__AUTH__TYPE"] = "jupyterhub"
        templates: |
          c.JupyterHub.template_paths.insert(0, "/etc/jupyterhub/templates")
        spawner: |
          #!/usr/bin/env python3

          from tornado import web

          EXPERIMENTAL_TAG = "2024.11.13.666"

          def enum(**enums):
              return type("Enum", (), enums)


          async def get_user_groups(self, user):
              # Retrieve user authentication info
              auth_state = await user.get_auth_state()
              # self.log.info("auth_state: %s", auth_state)

              try:
                auth0_info = auth_state.get("auth0_user")
                groups = auth0_info.get("org-groups", [])
                # self.log.info(f"groups: {groups}")
              except AttributeError:
                self.log.error("Could not retrieve user groups for %s", user.name)
                groups = []

              return groups

          async def custom_options_form(self):
              self.log.info(f"logged in user: {self.user.name}")

              user_groups = enum(
                  POWER_USERS="Power Users",
                  INTERNAL_USERS="Internal Users",
              )

              base_image = "ghcr.io/example/org-analytics-lab-image"

              #
              # PROFILE CONFIGURATION
              #

              # Default profiles for all users
              default_profile_list = [
                  {
                      "display_name": "Image - 30 GB",
                      "description": "4 Cores, 30 GB Memory",
                      "kubespawner_override": {
                          "cpu_guarantee": 3,
                          "cpu_limit": 4,
                          "mem_guarantee": "25G",
                          "mem_limit": "30G",
                      },
                  },
                  {
                      "display_name": "Image - 60 GB",
                      "description": "8 Cores, 60 GB Memory",
                      "kubespawner_override": {
                          "cpu_guarantee": 7,
                          "cpu_limit": 8,
                          "mem_guarantee": "50G",
                          "mem_limit": "60G",
                      },
                  },
                  {
                      "display_name": "Image with GPU - 30 GB",
                      "description": "1 NVIDIA GPU, 8 Cores, 30 GB Memory",
                      "kubespawner_override": {
                          "cpu_guarantee": 7,
                          "cpu_limit": 8,
                          "mem_guarantee": "25G",
                          "mem_limit": "30G",
                          "extra_resource_limits": {
                              "nvidia.com/gpu": "1"
                          },
                          "tolerations": [
                              {
                                  "key": "nvidia.com/gpu",
                                  "operator": "Exists",
                                  "effect": "NoSchedule",
                              }
                          ],
                      },
                  },
              ]

              self.profile_list = default_profile_list

              # Power user profiles
              power_user_profile_list = [
                  {
                      "display_name": "Image - 120 GB",
                      "description": "16 Cores, 120 GB Memory",
                      "kubespawner_override": {
                          "cpu_guarantee": 15,
                          "cpu_limit": 16,
                          "mem_guarantee": "100G",
                          "mem_limit": "120G",
                      },
                  },
                  {
                      "display_name": "Image - 240 GB",
                      "description": "32 Cores, 120 GB Memory",
                      "kubespawner_override": {
                          "cpu_guarantee": 31,
                          "cpu_limit": 32,
                          "mem_guarantee": "200G",
                          "mem_limit": "240G",
                      },
                  },
              ]

              # Internal user profiles
              internal_user_profile_list = [
                  {
                      "display_name": f"Unstable {EXPERIMENTAL_TAG}",
                      "description": "A testing environment used internally with 8 cores and 60 GB",
                      "kubespawner_override": {
                          "image": f"{base_image}:{EXPERIMENTAL_TAG}",
                          "cpu_guarantee": 7,
                          "cpu_limit": 8,
                          "mem_guarantee": "50G",
                          "mem_limit": "60G",
                      },
                  },
                  {
                      "display_name": f"Big Unstable {EXPERIMENTAL_TAG}",
                      "description": "A testing environment used internally with 32 cores and 240 GB",
                      "kubespawner_override": {
                          "image": f"{base_image}:{EXPERIMENTAL_TAG}",
                          "cpu_guarantee": 31,
                          "cpu_limit": 32,
                          "mem_guarantee": "200G",
                          "mem_limit": "240G",
                      },
                  },
              ]

              try:
                  # Read user access token to collect user group info
                  groups = await get_user_groups(self, self.user)
                  # self.log.info(f"groups: {groups}")
                  self.log.info(
                      f"{self.user.name} user belongs to group(s): {(','.join(groups))}"
                  )

                  if user_groups.POWER_USERS in groups:
                      self.profile_list.extend(power_user_profile_list)

                  if user_groups.INTERNAL_USERS in groups:
                      self.profile_list.extend(internal_user_profile_list)

                  # Set extra labels
                  self.extra_labels = {
                      "test": "alex-testing",
                  }

                  # Return options_form
                  return self._options_form_default()
              except Exception as e:
                  self.log.error(f"Exception: {e}")
                  raise web.HTTPError(400, "Something went wrong. Coud not load profiles")


          # Set the log level by value or name
          c.JupyterHub.log_level = "DEBUG"

          # Set cookies - jupyterhub-session-id and jupyterhub-hub-login - to less than a day
          c.Jupyterhub.cookie_max_age_days = 0.90
          c.JupyterHub.tornado_settings["cookie_options"] = dict(expires_days=0.90)

          # Enable debug-logging of the single-user server
          c.Spawner.debug = True

          # Enable debug-logging of the single-user server
          c.LocalProcessSpawner.debug = True
          c.Spawner.cmd = ["jupyterhub-singleuser"]

          # # Override spawner timeout - in seconds
          # c.KubeSpawner.start_timeout = 600
          # c.KubeSpawner.http_timeout = 60

          # Override options_form
          c.KubeSpawner.options_form = custom_options_form

      extraFiles: {}
      extraEnv: {}
      extraContainers: []
      extraVolumes:
      - name: hub-templates
        configMap:
          name: hub-templates
      extraVolumeMounts:
      - name: hub-templates
        mountPath: /etc/jupyterhub/templates
      image:
        name: quay.io/jupyterhub/k8s-hub
        tag: "3.3.6"
        pullPolicy:
        pullSecrets: []
      resources: {}
      podSecurityContext:
        fsGroup: 1000
      containerSecurityContext:
        runAsUser: 1000
        runAsGroup: 1000
        allowPrivilegeEscalation: false
      lifecycle: {}
      loadRoles: {}
      services: {}
      pdb:
        enabled: false
        maxUnavailable:
        minAvailable: 1
      networkPolicy:
        enabled: true
        ingress: []
        egress: []
        egressAllowRules:
          cloudMetadataServer: true
          dnsPortsCloudMetadataServer: true
          dnsPortsKubeSystemNamespace: true
          dnsPortsPrivateIPs: true
          nonPrivateIPs: true
          privateIPs: true
        interNamespaceAccessLabels: ignore
        allowedIngressPorts: []
      allowNamedServers: false
      namedServerLimitPerUser:
      authenticatePrometheus:
      redirectToServer:
      shutdownOnLogout:
      templatePaths: []
      templateVars: {}
      livenessProbe:
        # The livenessProbe's aim to give JupyterHub sufficient time to startup but
        # be able to restart if it becomes unresponsive for ~5 min.
        enabled: true
        initialDelaySeconds: 300
        periodSeconds: 10
        failureThreshold: 30
        timeoutSeconds: 3
      readinessProbe:
        # The readinessProbe's aim is to provide a successful startup indication,
        # but following that never become unready before its livenessProbe fail and
        # restarts it if needed. To become unready following startup serves no
        # purpose as there are no other pod to fallback to in our non-HA deployment.
        enabled: true
        initialDelaySeconds: 0
        periodSeconds: 2
        failureThreshold: 1000
        timeoutSeconds: 1
      existingSecret:
      serviceAccount:
        create: true
        name:
        annotations: {}
      extraPodSpec: {}

    rbac:
      create: true

    # proxy relates to the proxy pod, the proxy-public service, and the autohttps
    # pod and proxy-http service.
    proxy:
      # TODO: Pull this out into an AWS Secret
      secretToken:
      annotations: {}
      deploymentStrategy:
        ## type: Recreate
        ## - JupyterHub's interaction with the CHP proxy becomes a lot more robust
        ##   with this configuration. To understand this, consider that JupyterHub
        ##   during startup will interact a lot with the k8s service to reach a
        ##   ready proxy pod. If the hub pod during a helm upgrade is restarting
        ##   directly while the proxy pod is making a rolling upgrade, the hub pod
        ##   could end up running a sequence of interactions with the old proxy pod
        ##   and finishing up the sequence of interactions with the new proxy pod.
        ##   As CHP proxy pods carry individual state this is very error prone. One
        ##   outcome when not using Recreate as a strategy has been that user pods
        ##   have been deleted by the hub pod because it considered them unreachable
        ##   as it only configured the old proxy pod but not the new before trying
        ##   to reach them.
        type: Recreate
        ## rollingUpdate:
        ## - WARNING:
        ##   This is required to be set explicitly blank! Without it being
        ##   explicitly blank, k8s will let eventual old values under rollingUpdate
        ##   remain and then the Deployment becomes invalid and a helm upgrade would
        ##   fail with an error like this:
        ##
        ##     UPGRADE FAILED
        ##     Error: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
        ##     Error: UPGRADE FAILED: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
        rollingUpdate: # service relates to the proxy-public service
      service:
        type: ClusterIP
        labels: {}
        annotations: {}
        nodePorts:
          http:
          https:
        disableHttpPort: false
        extraPorts: []
        loadBalancerIP:
        loadBalancerSourceRanges: []
      # chp relates to the proxy pod, which is responsible for routing traffic based
      # on dynamic configuration sent from JupyterHub to CHP's REST API.
      chp:
        revisionHistoryLimit:
        containerSecurityContext:
          runAsUser: 65534 # nobody user
          runAsGroup: 65534 # nobody group
          allowPrivilegeEscalation: false
        image:
          name: quay.io/jupyterhub/configurable-http-proxy
          # tag is automatically bumped to new patch versions by the
          # watch-dependencies.yaml workflow.
          #
          tag: "4.6.1" # https://github.com/jupyterhub/configurable-http-proxy/tags
          pullPolicy:
          pullSecrets: []
        extraCommandLineFlags: []
        livenessProbe:
          enabled: true
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 30
          timeoutSeconds: 3
        readinessProbe:
          enabled: true
          initialDelaySeconds: 0
          periodSeconds: 2
          failureThreshold: 1000
          timeoutSeconds: 1
        resources: {}
        defaultTarget:
        errorTarget:
        extraEnv: {}
        nodeSelector: {}
        tolerations: []
        networkPolicy:
          enabled: true
          ingress: []
          egress: []
          egressAllowRules:
            cloudMetadataServer: true
            dnsPortsCloudMetadataServer: true
            dnsPortsKubeSystemNamespace: true
            dnsPortsPrivateIPs: true
            nonPrivateIPs: true
            privateIPs: true
          interNamespaceAccessLabels: ignore
          allowedIngressPorts: [ http, https ]
        pdb:
          enabled: false
          maxUnavailable:
          minAvailable: 1
        extraPodSpec: {}
      # traefik relates to the autohttps pod, which is responsible for TLS
      # termination when proxy.https.type=letsencrypt.
      traefik:
        revisionHistoryLimit:
        containerSecurityContext:
          runAsUser: 65534 # nobody user
          runAsGroup: 65534 # nobody group
          allowPrivilegeEscalation: false
        image:
          name: traefik
          # tag is automatically bumped to new patch versions by the
          # watch-dependencies.yaml workflow.
          #
          tag: "v2.11.0" # ref: https://hub.docker.com/_/traefik?tab=tags
          pullPolicy:
          pullSecrets: []
        hsts:
          includeSubdomains: false
          preload: false
          maxAge: 15724800 # About 6 months
        resources: {}
        labels: {}
        extraInitContainers: []
        extraEnv: {}
        extraVolumes: []
        extraVolumeMounts: []
        extraStaticConfig: {}
        extraDynamicConfig: {}
        nodeSelector: {}
        tolerations: []
        extraPorts: []
        networkPolicy:
          enabled: true
          ingress: []
          egress: []
          egressAllowRules:
            cloudMetadataServer: true
            dnsPortsCloudMetadataServer: true
            dnsPortsKubeSystemNamespace: true
            dnsPortsPrivateIPs: true
            nonPrivateIPs: true
            privateIPs: true
          interNamespaceAccessLabels: ignore
          allowedIngressPorts: [ http, https ]
        pdb:
          enabled: false
          maxUnavailable:
          minAvailable: 1
        serviceAccount:
          create: true
          name:
          annotations: {}
        extraPodSpec: {}
      secretSync:
        containerSecurityContext:
          runAsUser: 65534 # nobody user
          runAsGroup: 65534 # nobody group
          allowPrivilegeEscalation: false
        image:
          name: quay.io/jupyterhub/k8s-secret-sync
          tag: "3.3.6"
          pullPolicy:
          pullSecrets: []
        resources: {}
      labels: {}
      https:
        enabled: true
        type: offload

    # singleuser relates to the configuration of KubeSpawner which runs in the hub
    # pod, and its spawning of user pods such as jupyter-myusername.
    singleuser:
      podNameTemplate:
      extraTolerations: []
      nodeSelector:
        karpenter.sh/capacity-type: on-demand
      extraNodeAffinity:
        required: []
        preferred: []
      extraPodAffinity:
        required: []
        preferred: []
      extraPodAntiAffinity:
        required: []
        preferred: []
      networkTools:
        image:
          name: quay.io/jupyterhub/k8s-network-tools
          tag: "3.3.6"
          pullPolicy:
          pullSecrets: []
        resources: {}
      cloudMetadata:
        # block set to true will append a privileged initContainer using the
        # iptables to block the sensitive metadata server at the provided ip.
        blockWithIptables: true
        ip: 169.254.169.254
      networkPolicy:
        enabled: true
        ingress: []
        egress: []
        egressAllowRules:
          cloudMetadataServer: false
          dnsPortsCloudMetadataServer: true
          dnsPortsKubeSystemNamespace: true
          dnsPortsPrivateIPs: true
          nonPrivateIPs: true
          privateIPs: false
        interNamespaceAccessLabels: ignore
        allowedIngressPorts: []
      events: true
      extraAnnotations: {}
      extraLabels:
        hub.jupyter.org/network-access-hub: "true"
      extraFiles: {}
      extraEnv:
        AWS_NO_SIGN_REQUEST: NO
        # DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: "{JUPYTER_IMAGE}"
        # DASK_DISTRIBUTED__DASHBOARD__LINK: "{JUPYTERHUB_SERVICE_PREFIX}proxy/{port}/status"
        DASK_LABEXTENSION__FACTORY__MODULE: "dask_gateway"
        DASK_LABEXTENSION__FACTORY__CLASS: "GatewayCluster"
        # GDAL / Rasterio environment variables for performance
        GDAL_DISABLE_READDIR_ON_OPEN: "EMPTY_DIR"
        GDAL_HTTP_MERGE_CONSECUTIVE_RANGES: "YES"
        # Retry on Blob Storage errors
        GDAL_HTTP_MAX_RETRY: "5"
        GDAL_HTTP_RETRY_DELAY: "3"
        # Prefer shapely 2.0 to pygeos
        USE_PYGEOS: "0"

      lifecycleHooks:
        postStart:
          exec:
            command:
            - "bash"
            - "/etc/singleuser/k8s-lifecycle-hook-post-start.sh"

      initContainers: []
      extraContainers: []
      allowPrivilegeEscalation: false
      uid: 1000
      fsGid: 100
      serviceAccountName: user-read
      storage:
        type: dynamic
        extraLabels: {}
        extraVolumes:
        - name: user-etc-singleuser
          configMap:
            name: user-etc-singleuser
        - name: etc-ssh-sshconfigd
          configMap:
            name: etc-ssh-sshconfigd

        extraVolumeMounts:
        - name: user-etc-singleuser
          mountPath: /etc/singleuser
        - name: etc-ssh-sshconfigd
          mountPath: /etc/ssh/ssh_config.d/
          readOnly: true

        static:
          pvcName:
          subPath: "{username}"
        capacity: 10Gi
        homeMountPath: /home/jovyan
        dynamic:
          storageClass: gp2
          pvcNameTemplate: claim-{username}
          volumeNameTemplate: volume-{username}
          storageAccessModes: [ ReadWriteOnce ]
      image:
        name: ghcr.io/example/org-analytics-lab-image # {"$imagepolicy": "flux-system:jupyterhub:name"}
        tag: "2025.03.28" # {"$imagepolicy": "flux-system:jupyterhub:tag"}
        pullPolicy:
        pullSecrets: []
      startTimeout: 300
      cpu:
        limit: 2
        guarantee: 2
      memory:
        limit: 10G
        guarantee: 10G
      extraResource:
        limits: {}
        guarantees: {}
      cmd: jupyterhub-singleuser
      defaultUrl: "/lab/tree/notebooks/README.md"
      extraPodConfig: {}
      profileList:
      - display_name: "Jupyter - 15 GB"
        description: "Jupyter environment"
        kubespawner_override:
          cpu_guarantee: 3
          cpu_limit: 4
          mem_guarantee: 15G
          mem_limit: 15G
        default: true
      # Other profiles are defined below and are allocated based on user group

      # scheduling relates to the user-scheduler pods and user-placeholder pods.
    scheduling:
      userScheduler:
        enabled: true
        revisionHistoryLimit:
        replicas: 2
        logLevel: 4
        plugins:
          score:
            disabled:
            # We disable these plugins (with regards to scoring) to not interfere
            # or complicate our use of NodeResourcesFit.
            - name: NodeResourcesBalancedAllocation
            # Disable plugins to be allowed to enable them again with a different
            # weight and avoid an error.
            - name: NodeAffinity
            - name: InterPodAffinity
            - name: NodeResourcesFit
            - name: ImageLocality
            enabled:
            - name: NodeAffinity
              weight: 14631
            - name: InterPodAffinity
              weight: 1331
            - name: NodeResourcesFit
              weight: 121
            - name: ImageLocality
              weight: 11
        pluginConfig:
        # Here we declare that we should optimize pods to fit based on a
        # MostAllocated strategy instead of the default LeastAllocated.
        - name: NodeResourcesFit
          args:
            scoringStrategy:
              resources:
              - name: cpu
                weight: 1
              - name: memory
                weight: 1
              type: MostAllocated
        containerSecurityContext:
          runAsUser: 65534 # nobody user
          runAsGroup: 65534 # nobody group
          allowPrivilegeEscalation: false
        image:
          # Alex pinned this version to 1.26, due to an error with later versions
          name: registry.k8s.io/kube-scheduler
          tag: "v1.26.14" # ref: https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG
          pullPolicy:
          pullSecrets: []
        nodeSelector: {}
        tolerations: []
        labels: {}
        annotations: {}
        pdb:
          enabled: true
          maxUnavailable: 1
          minAvailable:
        resources: {}
        serviceAccount:
          create: true
          name:
          annotations: {}
        extraPodSpec: {}
      podPriority:
        enabled: false
        globalDefault: false
        defaultPriority: 0
        imagePullerPriority: -5
        userPlaceholderPriority: -10
      userPlaceholder:
        enabled: true
        image:
          name: registry.k8s.io/pause
          # tag is automatically bumped to new patch versions by the
          # watch-dependencies.yaml workflow.
          #
          # If you update this, also update prePuller.pause.image.tag
          #
          tag: "3.9"
          pullPolicy:
          pullSecrets: []
        revisionHistoryLimit:
        replicas: 0
        labels: {}
        annotations: {}
        containerSecurityContext:
          runAsUser: 65534 # nobody user
          runAsGroup: 65534 # nobody group
          allowPrivilegeEscalation: false
        resources: {}
      corePods:
        tolerations:
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
          effect: NoSchedule
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
          effect: NoSchedule
        nodeAffinity:
          matchNodePurpose: prefer
      userPods:
        tolerations:
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
          effect: NoSchedule
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
          effect: NoSchedule
        nodeAffinity:
          matchNodePurpose: prefer

    # prePuller relates to the hook|continuous-image-puller DaemonsSets
    prePuller:
      revisionHistoryLimit:
      labels: {}
      annotations: {}
      resources: {}
      containerSecurityContext:
        runAsUser: 65534 # nobody user
        runAsGroup: 65534 # nobody group
        allowPrivilegeEscalation: false
      extraTolerations: []
      # hook relates to the hook-image-awaiter Job and hook-image-puller DaemonSet
      hook:
        enabled: true
        pullOnlyOnChanges: true
        # image and the configuration below relates to the hook-image-awaiter Job
        image:
          name: quay.io/jupyterhub/k8s-image-awaiter
          tag: "3.3.6"
          pullPolicy:
          pullSecrets: []
        containerSecurityContext:
          runAsUser: 65534 # nobody user
          runAsGroup: 65534 # nobody group
          allowPrivilegeEscalation: false
        podSchedulingWaitDuration: 10
        nodeSelector: {}
        tolerations: []
        resources: {}
        serviceAccount:
          create: true
          name:
          annotations: {}
      continuous:
        enabled: true
      pullProfileListImages: true
      extraImages: {}
      pause:
        containerSecurityContext:
          runAsUser: 65534 # nobody user
          runAsGroup: 65534 # nobody group
          allowPrivilegeEscalation: false
        image:
          name: registry.k8s.io/pause
          # tag is automatically bumped to new patch versions by the
          # watch-dependencies.yaml workflow.
          #
          # If you update this, also update scheduling.userPlaceholder.image.tag
          #
          tag: "3.9"
          pullPolicy:
          pullSecrets: []

    # cull relates to the jupyterhub-idle-culler service, responsible for evicting
    # inactive singleuser pods.
    #
    # The configuration below, except for enabled, corresponds to command-line flags
    # for jupyterhub-idle-culler as documented here:
    # https://github.com/jupyterhub/jupyterhub-idle-culler#as-a-standalone-script
    #
    cull:
      enabled: true
      users: false # --cull-users
      adminUsers: false # --cull-admin-users
      removeNamedServers: false # --remove-named-servers
      timeout: 3600 # --timeout
      every: 600 # --cull-every
      concurrency: 10 # --concurrency
      maxAge: 0 # --max-age

    debug:
      enabled: false

    global:
      safeToShowValues: false
